{"changed":true,"filter":false,"title":"mars_env.py","tooltip":"/simulation_ws/src/rl-agent/markov/environments/mars_env.py","value":"from __future__ import print_function\n\nimport time\nimport boto3\nimport gym\nimport numpy as np\nfrom gym import spaces\nimport PIL\nfrom PIL import Image\nimport os\nimport random\nimport math\nimport sys\nimport rospy\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import Twist, Pose, Quaternion\nfrom gazebo_msgs.srv import SetModelState\nfrom gazebo_msgs.msg import ModelState, ContactsState\nfrom sensor_msgs.msg import Image as sensor_image\nfrom sensor_msgs.msg import LaserScan, Imu\nfrom geometry_msgs.msg import Point\nfrom std_msgs.msg import Float64\nfrom std_msgs.msg import String\nfrom PIL import Image\nimport queue\n\n\nVERSION = \"0.0.1\"\nTRAINING_IMAGE_WIDTH = 160\nTRAINING_IMAGE_HEIGHT = 120\nTRAINING_IMAGE_SIZE = (TRAINING_IMAGE_WIDTH, TRAINING_IMAGE_HEIGHT)\n\nLIDAR_SCAN_MAX_DISTANCE = 4.5  # Max distance Lidar scanner can measure\nCRASH_DISTANCE = 0.49  # Min distance to obstacle (The LIDAR is in the center of the 1M Rover)\n\n# Size of the image queue buffer, we want this to be one so that we consume 1 image\n# at a time, but may want to change this as we add more algorithms\nIMG_QUEUE_BUF_SIZE = 1\n\n# Prevent unknown \"stuck\" scenarios with a kill switch (MAX_STEPS)\nMAX_STEPS = 2000\n\n# Destination Point\nCHECKPOINT_X = 44.25\nCHECKPOINT_Y = -4\n\n# Initial position of the robot\nINITIAL_POS_X = -0.170505086911\nINITIAL_POS_Y = 0.114341186761\nINITIAL_POS_Z = -0.0418765865136\n\nINITIAL_ORIENT_X = 0.0135099011407\nINITIAL_ORIENT_Y = 0.040927747122\nINITIAL_ORIENT_Z = 0.0365547169101\nINITIAL_ORIENT_W = 0.998401800258\n\n\n# Initial distance to checkpoint\nINITIAL_DISTANCE_TO_CHECKPOINT = abs(math.sqrt(((CHECKPOINT_X - INITIAL_POS_X) ** 2) +\n                                               ((CHECKPOINT_Y - INITIAL_POS_Y) ** 2)))\n\n\n# SLEEP INTERVALS - a buffer to give Gazebo, RoS and the rl_agent to sync.\nSLEEP_AFTER_RESET_TIME_IN_SECOND = 0.3\nSLEEP_BETWEEN_ACTION_AND_REWARD_CALCULATION_TIME_IN_SECOND = 0.3 # LIDAR Scan is 5 FPS (0.2sec).\nSLEEP_WAITING_FOR_IMAGE_TIME_IN_SECOND = 0.01\n\n\nclass MarsEnv(gym.Env):\n    def __init__(self):\n        self.x = INITIAL_POS_X                                                  # Current position of Rover \n        self.y = INITIAL_POS_Y                                                  # Current position of Rover\n        self.last_position_x = INITIAL_POS_X                                    # Previous position of Rover\n        self.last_position_y = INITIAL_POS_Y                                    # Previous position of Rover\n        #self.orientation = None\n        self.aws_region = os.environ.get(\"AWS_REGION\", \"us-east-1\")             # Region for CloudWatch Metrics\n        self.reward_in_episode = 0                                              # Global episodic reward variable\n        self.steps = 0                                                          # Global episodic step counter\n        self.collision_threshold = sys.maxsize                                  # current collision distance\n        self.last_collision_threshold = sys.maxsize                             # previous collision distance\n        self.collision = False                                                  # Episodic collision detector\n        self.distance_travelled = 0                                             # Global episodic distance counter\n        self.current_distance_to_checkpoint = INITIAL_DISTANCE_TO_CHECKPOINT    # current distance to checkpoint\n        self.closer_to_checkpoint = False                                       # Was last step closer to checkpoint?\n        self.state = None                                                       # Observation space\n        self.steering = 0\n        self.throttle = 0\n        self.power_supply_range = MAX_STEPS                                     # Kill switch (power supply)\n        \n\n        # Imu Sensor readings\n        self.max_lin_accel_x = 0\n        self.max_lin_accel_y = 0\n        self.max_lin_accel_z = 0\n        \n        self.reached_waypoint_1 = False\n        self.reached_waypoint_2 = False\n        self.reached_waypoint_3 = False\n\n\n        # action space -> steering angle, throttle\n        self.action_space = spaces.Box(low=np.array([-1, 0]), high=np.array([+1, +3]), dtype=np.float32)\n\n\n        # Create the observation space\n        self.observation_space = spaces.Box(low=0, high=255,\n                                            shape=(TRAINING_IMAGE_SIZE[1], TRAINING_IMAGE_SIZE[0], 3),\n                                            dtype=np.uint8)\n\n        self.image_queue = queue.Queue(IMG_QUEUE_BUF_SIZE)\n\n        # ROS initialization\n        self.ack_publisher = rospy.Publisher('/cmd_vel', Twist, queue_size=100)\n\n        # ROS Subscriptions\n        self.current_position_pub = rospy.Publisher('/current_position', Point, queue_size=3)\n        self.distance_travelled_pub = rospy.Publisher('/distance_travelled', String, queue_size=3)\n        # ################################################################################\n\n        # Gazebo model state\n        self.gazebo_model_state_service = rospy.ServiceProxy('/gazebo/set_model_state', SetModelState)\n        rospy.init_node('rl_coach', anonymous=True)\n\n        # Subscribe to ROS topics and register callbacks\n        rospy.Subscriber('/odom', Odometry, self.callback_pose)\n        rospy.Subscriber('/scan', LaserScan, self.callback_scan)\n        rospy.Subscriber('/robot_bumper', ContactsState, self.callback_collision)\n        rospy.Subscriber('/camera/image_raw', sensor_image, self.callback_image)\n        # IMU Sensors\n        rospy.Subscriber('/imu/wheel_lb', Imu, self.callback_wheel_lb)\n\n\n\n    '''\n    DO NOT EDIT - Function called by rl_coach to instruct the agent to take an action\n    '''\n    def step(self, action):\n        # initialize rewards, next_state, done\n        self.reward = None\n        self.done = False\n        self.next_state = None\n\n        steering = float(action[0])\n        throttle = float(action[1])\n        self.steps += 1\n        self.send_action(steering, throttle)\n        time.sleep(SLEEP_BETWEEN_ACTION_AND_REWARD_CALCULATION_TIME_IN_SECOND)\n\n        self.call_reward_function(action)\n\n        info = {}  # additional data, not to be used for training\n\n        return self.next_state, self.reward, self.done, info\n\n\n    '''\n    DO NOT EDIT - Function called at the conclusion of each episode to reset episodic values\n    '''\n    def reset(self):\n        print('Total Episodic Reward=%.2f' % self.reward_in_episode,\n              'Total Episodic Steps=%.2f' % self.steps)\n        self.send_reward_to_cloudwatch(self.reward_in_episode)\n\n        # Reset global episodic values\n        self.reward = None\n        self.done = False\n        self.next_state = None\n        self.ranges= None\n        self.send_action(0, 0) # set the throttle to 0\n        self.rover_reset()\n        self.call_reward_function([0, 0])\n\n        return self.next_state\n\n\n    '''\n    DO NOT EDIT - Function called to send the agent's chosen action to the simulator (Gazebo)\n    '''\n    def send_action(self, steering, throttle):\n        speed = Twist()\n        speed.linear.x = throttle\n        speed.angular.z = steering\n        self.ack_publisher.publish(speed)\n\n\n    '''\n    DO NOT EDIT - Function to reset the rover to the starting point in the world\n    '''\n    def rover_reset(self):\n        \n        # Reset Rover-related Episodic variables\n        rospy.wait_for_service('gazebo/set_model_state')\n\n        self.x = INITIAL_POS_X\n        self.y = INITIAL_POS_Y\n\n        # Put the Rover at the initial position\n        model_state = ModelState()\n        model_state.pose.position.x = INITIAL_POS_X\n        model_state.pose.position.y = INITIAL_POS_Y\n        model_state.pose.position.z = INITIAL_POS_Z\n        model_state.pose.orientation.x = INITIAL_ORIENT_X\n        model_state.pose.orientation.y = INITIAL_ORIENT_Y\n        model_state.pose.orientation.z = INITIAL_ORIENT_Z\n        model_state.pose.orientation.w = INITIAL_ORIENT_W\n        model_state.twist.linear.x = 0\n        model_state.twist.linear.y = 0\n        model_state.twist.linear.z = 0\n        model_state.twist.angular.x = 0\n        model_state.twist.angular.y = 0\n        model_state.twist.angular.z = 0\n        model_state.model_name = 'rover'\n\n        self.gazebo_model_state_service(model_state)\n\n        self.last_collision_threshold = sys.maxsize\n        self.last_position_x = self.x\n        self.last_position_y = self.y\n\n        time.sleep(SLEEP_AFTER_RESET_TIME_IN_SECOND)\n\n        self.distance_travelled = 0\n        self.current_distance_to_checkpoint = INITIAL_DISTANCE_TO_CHECKPOINT\n        self.steps = 0\n        self.reward_in_episode = 0\n        self.collision = False\n        self.closer_to_checkpoint = False\n        self.power_supply_range = MAX_STEPS\n        self.max_lin_accel_x = 0\n        self.max_lin_accel_y = 0\n        self.max_lin_accel_z = 0\n        \n        # First clear the queue so that we set the state to the start image\n        _ = self.image_queue.get(block=True, timeout=None)\n        self.set_next_state()\n\n    '''\n    DO NOT EDIT - Function to find the distance between the rover and nearest object within 4.5M via LIDAR\n    '''\n    def get_distance_to_object(self):\n\n        while not self.ranges:\n            time.sleep(SLEEP_WAITING_FOR_IMAGE_TIME_IN_SECOND)\n\n        size = len(self.ranges)\n        x = np.linspace(0, size - 1, 360)\n        xp = np.arange(size)\n        val = np.clip(np.interp(x, xp, self.ranges), 0, LIDAR_SCAN_MAX_DISTANCE)\n        val[np.isnan(val)] = LIDAR_SCAN_MAX_DISTANCE\n\n        # Find min distance\n        self.collision_threshold = np.amin(val)\n\n\n    '''\n    DO NOT EDIT - Function to resize the image from the camera and set observation_space\n    '''\n    def set_next_state(self):\n        try:\n            # Make sure the first image is the starting image\n            image_data = self.image_queue.get(block=True, timeout=None)\n        \n            # Read the image and resize to get the state\n            image = Image.frombytes('RGB', (image_data.width, image_data.height), image_data.data, 'raw', 'RGB', 0, 1)\n            image = image.resize((TRAINING_IMAGE_WIDTH,TRAINING_IMAGE_HEIGHT), PIL.Image.ANTIALIAS)\n            \n            # TODO - can we crop this image to get additional savings?\n           \n            self.next_state = np.array(image)\n        except Exception as err:\n            print(\"Error!::set_next_state:: {}\".format(err))\n\n\n    '''\n    DO NOT EDIT - Reward Function buffer\n    '''\n    def call_reward_function(self, action):\n        self.get_distance_to_object() #<-- Also evaluate for sideswipe and collistion damage\n        \n        # Get the observation\n        self.set_next_state()\n        \n        # reduce power supply range\n        self.power_supply_range = MAX_STEPS - self.steps\n        \n        # calculate reward\n        reward, done = self.reward_function()\n\n        # Accumulate reward for the episode\n        self.reward_in_episode += reward\n\n        # Get average Imu reading\n        if self.max_lin_accel_x > 0 or self.max_lin_accel_y > 0 or self.max_lin_accel_z > 0:\n            avg_imu = (self.max_lin_accel_x + self.max_lin_accel_y + self.max_lin_accel_y) / 3\n        else:\n            avg_imu = 0\n    \n        print('Step:%.2f' % self.steps,\n              'Steering:%f' % action[0],\n              'R:%.2f' % reward,                                # Reward\n              'DTCP:%f' % self.current_distance_to_checkpoint,  # Distance to Check Point\n              'DT:%f' % self.distance_travelled,                # Distance Travelled\n              'CT:%.2f' % self.collision_threshold,             # Collision Threshold\n              'CTCP:%f' % self.closer_to_checkpoint,            # Is closer to checkpoint\n              'PSR: %f' % self.power_supply_range,              # Steps remaining in Episode\n              'IMU: %f' % avg_imu)\n\n        self.reward = reward\n        self.done = done\n\n        self.last_position_x = self.x\n        self.last_position_y = self.y\n\n\n\n    '''\n    EDIT - but do not change the method signature. \n    Must return a reward value as a float \n    Must return a boolean value indicating if episode is complete\n    Must be returned in order of reward, done\n    '''\n    def reward_function(self):\n        '''\n        :return: reward as float\n                 done as boolean\n        '''\n        \n        reward = 0\n        done = false\n            \n        \n        return reward, done\n    \n        \n    '''\n    DO NOT EDIT - Function to receive LIDAR data from a ROSTopic\n    '''\n    def callback_scan(self, data):\n        self.ranges = data.ranges\n\n\n    '''\n    DO NOT EDIT - Function to receive image data from the camera RoSTopic\n    '''\n    def callback_image(self, data):\n        try:\n             self.image_queue.put_nowait(data)\n        except queue.Full:\n            pass\n        except Exception as ex:\n           print(\"Error! {}\".format(ex))\n\n\n    '''\n    DO NOT EDIT - Function to receive IMU data from the Rover wheels\n    '''\n    def callback_wheel_lb(self, data):\n        lin_accel_x = data.linear_acceleration.x\n        lin_accel_y = data.linear_acceleration.x\n        lin_accel_z = data.linear_acceleration.x\n\n        if lin_accel_x > self.max_lin_accel_x:\n            self.max_lin_accel_x = lin_accel_x\n\n        if lin_accel_y > self.max_lin_accel_y:\n            self.max_lin_accel_y = lin_accel_y\n\n        if lin_accel_z > self.max_lin_accel_z:\n            self.max_lin_accel_z = lin_accel_z\n\n\n\n    '''\n    DO NOT EDIT - Function to receive Position/Orientation data from a ROSTopic\n    '''\n    def callback_pose(self, data):\n        #self.orientation = data.pose.pose.orientation\n        self.linear_trajectory = data.twist.twist.linear\n        self.angular_trajectory = data.twist.twist.angular\n\n        new_position = data.pose.pose.position\n\n        p = Point(new_position.x, new_position.y, new_position.z)\n\n        # Publish current position\n        self.current_position_pub.publish(p)\n\n        # Calculate total distance travelled\n        dist = math.hypot(new_position.x - self.x, new_position.y - self.y)\n        self.distance_travelled += dist\n        \n        # Calculate the distance to checkpoint\n        new_distance_to_checkpoint = Float64\n        new_distance_to_checkpoint.data = abs(math.sqrt(((new_position.x - CHECKPOINT_X) ** 2) +\n                                                        (new_position.y - CHECKPOINT_Y) ** 2))\n\n        if new_distance_to_checkpoint.data < self.current_distance_to_checkpoint:\n            self.closer_to_checkpoint = True\n        else:\n            self.closer_to_checkpoint = False\n\n        # Update the distance to checkpoint\n        self.current_distance_to_checkpoint = new_distance_to_checkpoint.data\n\n        # update the current position\n        self.x = new_position.x\n        self.y = new_position.y\n\n    \n    '''\n    DO NOT EDIT - Function to receive Collision data from a ROSTopic\n    '''\n    def callback_collision(self, data):\n        # Listen for a collision with anything in the environment\n        collsion_states = data.states\n        if len(collsion_states) > 0:\n            self.collide = True\n\n    \n    '''\n    DO NOT EDIT - Function to wrote episodic rewards to CloudWatch\n    '''\n    def send_reward_to_cloudwatch(self, reward):\n        try:\n            session = boto3.session.Session()\n            cloudwatch_client = session.client('cloudwatch', region_name=self.aws_region)\n            cloudwatch_client.put_metric_data(\n                MetricData=[\n                    {\n                        'MetricName': 'Episode_Reward',\n                        'Unit': 'None',\n                        'Value': reward\n                    },\n                    {\n                        'MetricName': 'Episode_Steps',\n                        'Unit': 'None',\n                        'Value': self.steps,\n                    },\n                    {\n                        'MetricName': 'DistanceToCheckpoint',\n                        'Unit': 'None',\n                        'Value': self.current_distance_to_checkpoint\n                    }\n                ],\n                Namespace='AWS_NASA_JPL_OSR_Challenge'\n            )\n        except Exception as err:\n            print(\"Error in the send_reward_to_cloudwatch function: {}\".format(err))\n\n\n'''\nDO NOT EDIT - Inheritance class to convert discrete actions to continuous actions\n'''\nclass MarsDiscreteEnv(MarsEnv):\n    def __init__(self):\n        MarsEnv.__init__(self)\n        print(\"New Martian Gym environment created...\")\n        \n        # actions -> straight, left, right\n        self.action_space = spaces.Discrete(3)\n\n    def step(self, action):\n        # Convert discrete to continuous\n        if action == 0:  # turn left\n            steering = 1.0\n            throttle = 3.00\n        elif action == 1:  # turn right\n            steering = -1.0\n            throttle = 3.00\n        elif action == 2:  # straight\n            steering = 0\n            throttle = 3.00\n        else:  # should not be here\n            raise ValueError(\"Invalid action\")\n\n        continuous_action = [steering, throttle]\n\n        return super().step(continuous_action)","undoManager":{"mark":0,"position":11,"stack":[[{"start":{"row":19,"column":37},"end":{"row":19,"column":42},"action":"insert","lines":[", Imu"],"id":2,"ignore":true},{"start":{"row":89,"column":0},"end":{"row":90,"column":0},"action":"insert","lines":["",""]},{"start":{"row":90,"column":8},"end":{"row":93,"column":32},"action":"insert","lines":["# Imu Sensor readings","        self.max_lin_accel_x = 0","        self.max_lin_accel_y = 0","        self.max_lin_accel_z = 0"]},{"start":{"row":114,"column":10},"end":{"row":114,"column":85},"action":"remove","lines":["TODO - Remove these publishers before release - they are for debugging only"]},{"start":{"row":114,"column":10},"end":{"row":114,"column":27},"action":"insert","lines":["ROS Subscriptions"]},{"start":{"row":119,"column":0},"end":{"row":120,"column":0},"action":"insert","lines":["        # Gazebo model state",""]},{"start":{"row":128,"column":0},"end":{"row":131,"column":0},"action":"insert","lines":["        # IMU Sensors","        rospy.Subscriber('/imu/wheel_lb', Imu, self.callback_wheel_lb)","",""]},{"start":{"row":231,"column":0},"end":{"row":234,"column":0},"action":"insert","lines":["        self.max_lin_accel_x = 0","        self.max_lin_accel_y = 0","        self.max_lin_accel_z = 0",""]},{"start":{"row":294,"column":0},"end":{"row":299,"column":0},"action":"insert","lines":["        # Get average Imu reading","        if self.max_lin_accel_x > 0 or self.max_lin_accel_y > 0 or self.max_lin_accel_z > 0:","            avg_imu = (self.max_lin_accel_x + self.max_lin_accel_y + self.max_lin_accel_y) / 3","        else:","            avg_imu = 0",""]},{"start":{"row":307,"column":49},"end":{"row":307,"column":50},"action":"remove","lines":[")"]},{"start":{"row":307,"column":49},"end":{"row":307,"column":50},"action":"insert","lines":[","]},{"start":{"row":307,"column":92},"end":{"row":308,"column":34},"action":"insert","lines":["","              'IMU: %f' % avg_imu)"]},{"start":{"row":492,"column":0},"end":{"row":493,"column":0},"action":"insert","lines":["",""]},{"start":{"row":493,"column":4},"end":{"row":511,"column":0},"action":"insert","lines":["'''","    DO NOT EDIT - Function to receive IMU data from the Rover wheels","    '''","    def callback_wheel_lb(self, data):","        lin_accel_x = data.linear_acceleration.x","        lin_accel_y = data.linear_acceleration.x","        lin_accel_z = data.linear_acceleration.x","","        if lin_accel_x > self.max_lin_accel_x:","            self.max_lin_accel_x = lin_accel_x","","        if lin_accel_y > self.max_lin_accel_y:","            self.max_lin_accel_y = lin_accel_y","","        if lin_accel_z > self.max_lin_accel_z:","            self.max_lin_accel_z = lin_accel_z","","",""]}],[{"start":{"row":330,"column":8},"end":{"row":468,"column":45},"action":"remove","lines":["# Corner boundaries of the world (in Meters)","        STAGE_X_MIN = -44.0","        STAGE_Y_MIN = -25.0","        STAGE_X_MAX = 15.0","        STAGE_Y_MAX = 22.0","        ","        ","        GUIDERAILS_X_MIN = -46","        GUIDERAILS_X_MAX = 1","        GUIDERAILS_Y_MIN = -6","        GUIDERAILS_Y_MAX = 4","        ","        ","        # WayPoints to checkpoint","        WAYPOINT_1_X = -10","        WAYPOINT_1_Y = -4","        ","        WAYPOINT_2_X = -17","        WAYPOINT_2_Y = 3","        ","        WAYPOINT_3_X = -34","        WAYPOINT_3_Y = 3","        ","        # REWARD Multipliers","        FINISHED_REWARD = 10000","        WAYPOINT_1_REWARD = 1000","        WAYPOINT_2_REWARD = 2000","        WAYPOINT_3_REWARD = 3000","","        reward = 0","        base_reward = 2","        multiplier = 0","        done = False","        ","        ","        if self.steps > 0:","            ","            # Check for episode ending events first","            # ###########################################","            ","            # Has LIDAR registered a hit","            if self.collision_threshold <= CRASH_DISTANCE:","                print(\"Rover has sustained sideswipe damage\")","                return 0, True # No reward","            ","            # Have the gravity sensors registered too much G-force","            if self.collision:","                print(\"Rover has collided with an object\")","                return 0, True # No reward","            ","            # Has the rover reached the max steps","            if self.power_supply_range < 1:","                print(\"Rover's power supply has been drained (MAX Steps reached\")","                return 0, True # No reward","            ","            # Has the Rover reached the destination","            if self.last_position_x >= CHECKPOINT_X and self.last_position_y >= CHECKPOINT_Y:","                print(\"Congratulations! The rover has reached the checkpoint!\")","                multiplier = FINISHED_REWARD","                reward = (base_reward * multiplier) / self.steps # <-- incentivize to reach checkpoint in fewest steps","                return reward, True","            ","            # If it has not reached the check point is it still on the map?","            if self.x < (GUIDERAILS_X_MIN - .45) or self.x > (GUIDERAILS_X_MAX + .45):","                print(\"Rover has left the mission map!\")","                return 0, True","                ","                ","            if self.y < (GUIDERAILS_Y_MIN - .45) or self.y > (GUIDERAILS_Y_MAX + .45):","                print(\"Rover has left the mission map!\")","                return 0, True","            ","            ","            # No Episode ending events - continue to calculate reward","            ","            if self.last_position_x <= WAYPOINT_1_X and self.last_position_y <= WAYPOINT_1_Y: # Rover is past the midpoint","                # Determine if Rover already received one time reward for reaching this waypoint","                if not self.reached_waypoint_1:  ","                    self.reached_waypoint_1 = True","                    print(\"Congratulations! The rover has reached waypoint 1!\")","                    multiplier = 1 ","                    reward = (WAYPOINT_1_REWARD * multiplier)/ self.steps # <-- incentivize to reach way-point in fewest steps","                    return reward, False","            ","            if self.last_position_x <= WAYPOINT_2_X and self.last_position_y >= WAYPOINT_2_Y: # Rover is past the midpoint","                # Determine if Rover already received one time reward for reaching this waypoint","                if not self.reached_waypoint_2:  ","                    self.reached_waypoint_2 = True","                    print(\"Congratulations! The rover has reached waypoint 2!\")","                    multiplier = 1 ","                    reward = (WAYPOINT_2_REWARD * multiplier)/ self.steps # <-- incentivize to reach way-point in fewest steps","                    return reward, False","                    ","            if self.last_position_x <= WAYPOINT_3_X and self.last_position_y >= WAYPOINT_3_Y: # Rover is past the midpoint","                # Determine if Rover already received one time reward for reaching this waypoint","                if not self.reached_waypoint_3:  ","                    self.reached_waypoint_3 = True","                    print(\"Congratulations! The rover has reached waypoint 3!\")","                    multiplier = 1 ","                    reward = (WAYPOINT_3_REWARD * multiplier)/ self.steps # <-- incentivize to reach way-point in fewest steps","                    return reward, False","                    ","            ","            # To reach this point in the function the Rover has either not yet reached the way-points OR has already gotten the one time reward for reaching the waypoint(s)","               ","            # multiply the reward based on the Rover's proximity to the Checkpoint","            waypoint_interval = INITIAL_DISTANCE_TO_CHECKPOINT / 5 ","           ","            marker = [waypoint_interval,(waypoint_interval * 2),(waypoint_interval * 3),(waypoint_interval * 4)]","                ","            # Get the Base multiplier","            if self.current_distance_to_checkpoint <= marker[0]:","                multiplier = 5","            elif self.current_distance_to_checkpoint <= marker[1] and self.current_distance_to_checkpoint > marker[0]:","                multiplier = 4","            elif self.current_distance_to_checkpoint <= marker[2] and self.current_distance_to_checkpoint > marker[1]:","                multiplier = 3","            elif self.current_distance_to_checkpoint <= marker[3] and self.current_distance_to_checkpoint > marker[2]:","                multiplier = 2","            else:","                multiplier = 1","            ","            # Incentivize the rover to stay away from objects","            if self.collision_threshold >= 2.0:      # very safe distance","                multiplier = multiplier + 1","            elif self.collision_threshold < 2.0 and self.collision_threshold >= 1.5: # pretty safe","                multiplier = multiplier + .5","            elif self.collision_threshold < 1.5 and self.collision_threshold >= 1.0: # just enough time to turn","                multiplier = multiplier + .25","            else:","                multiplier = multiplier # probably going to hit something and get a zero reward","            ","            # Incentize the rover to move towards the Checkpoint and not away from the checkpoint","            if not self.closer_to_checkpoint:","                if multiplier > 0:","                    # Cut the multiplier in half","                    multiplier = multiplier/2","                    ","            reward = base_reward * multiplier"],"id":3}],[{"start":{"row":330,"column":8},"end":{"row":330,"column":9},"action":"insert","lines":["r"],"id":4},{"start":{"row":330,"column":9},"end":{"row":330,"column":10},"action":"insert","lines":["e"]},{"start":{"row":330,"column":10},"end":{"row":330,"column":11},"action":"insert","lines":["w"]},{"start":{"row":330,"column":11},"end":{"row":330,"column":12},"action":"insert","lines":["a"]},{"start":{"row":330,"column":12},"end":{"row":330,"column":13},"action":"insert","lines":["r"]},{"start":{"row":330,"column":13},"end":{"row":330,"column":14},"action":"insert","lines":["d"]}],[{"start":{"row":330,"column":14},"end":{"row":330,"column":15},"action":"insert","lines":[" "],"id":5},{"start":{"row":330,"column":15},"end":{"row":330,"column":16},"action":"insert","lines":["="]}],[{"start":{"row":330,"column":16},"end":{"row":330,"column":17},"action":"insert","lines":[" "],"id":6},{"start":{"row":330,"column":17},"end":{"row":330,"column":18},"action":"insert","lines":["0"]}],[{"start":{"row":330,"column":18},"end":{"row":331,"column":0},"action":"insert","lines":["",""],"id":7},{"start":{"row":331,"column":0},"end":{"row":331,"column":8},"action":"insert","lines":["        "]},{"start":{"row":331,"column":8},"end":{"row":331,"column":9},"action":"insert","lines":["d"]},{"start":{"row":331,"column":9},"end":{"row":331,"column":10},"action":"insert","lines":["o"]},{"start":{"row":331,"column":10},"end":{"row":331,"column":11},"action":"insert","lines":["n"]},{"start":{"row":331,"column":11},"end":{"row":331,"column":12},"action":"insert","lines":["e"]}],[{"start":{"row":331,"column":12},"end":{"row":331,"column":13},"action":"insert","lines":[" "],"id":8},{"start":{"row":331,"column":13},"end":{"row":331,"column":14},"action":"insert","lines":["="]}],[{"start":{"row":331,"column":14},"end":{"row":331,"column":15},"action":"insert","lines":[" "],"id":9},{"start":{"row":331,"column":15},"end":{"row":331,"column":16},"action":"insert","lines":["d"]},{"start":{"row":331,"column":16},"end":{"row":331,"column":17},"action":"insert","lines":["a"]}],[{"start":{"row":331,"column":16},"end":{"row":331,"column":17},"action":"remove","lines":["a"],"id":10},{"start":{"row":331,"column":15},"end":{"row":331,"column":16},"action":"remove","lines":["d"]}],[{"start":{"row":331,"column":15},"end":{"row":331,"column":16},"action":"insert","lines":["f"],"id":11},{"start":{"row":331,"column":16},"end":{"row":331,"column":17},"action":"insert","lines":["a"]},{"start":{"row":331,"column":17},"end":{"row":331,"column":18},"action":"insert","lines":["l"]},{"start":{"row":331,"column":18},"end":{"row":331,"column":19},"action":"insert","lines":["s"]},{"start":{"row":331,"column":19},"end":{"row":331,"column":20},"action":"insert","lines":["e"]}],[{"start":{"row":319,"column":33},"end":{"row":319,"column":41},"action":"remove","lines":["function"],"id":12},{"start":{"row":319,"column":33},"end":{"row":319,"column":34},"action":"insert","lines":["m"]},{"start":{"row":319,"column":34},"end":{"row":319,"column":35},"action":"insert","lines":["e"]},{"start":{"row":319,"column":35},"end":{"row":319,"column":36},"action":"insert","lines":["t"]},{"start":{"row":319,"column":36},"end":{"row":319,"column":37},"action":"insert","lines":["h"]},{"start":{"row":319,"column":37},"end":{"row":319,"column":38},"action":"insert","lines":["o"]},{"start":{"row":319,"column":38},"end":{"row":319,"column":39},"action":"insert","lines":["d"]}],[{"start":{"row":228,"column":6},"end":{"row":230,"column":39},"action":"remove","lines":["  self.reached_waypoint_1 = False","        self.reached_waypoint_2 = False","        self.reached_waypoint_3 = False"],"id":13},{"start":{"row":228,"column":5},"end":{"row":228,"column":6},"action":"remove","lines":[" "]},{"start":{"row":228,"column":4},"end":{"row":228,"column":5},"action":"remove","lines":[" "]},{"start":{"row":228,"column":0},"end":{"row":228,"column":4},"action":"remove","lines":["    "]},{"start":{"row":227,"column":43},"end":{"row":228,"column":0},"action":"remove","lines":["",""]}]]},"ace":{"folds":[{"start":{"row":136,"column":27},"end":{"row":152,"column":60},"placeholder":"..."},{"start":{"row":178,"column":46},"end":{"row":182,"column":41},"placeholder":"..."},{"start":{"row":257,"column":29},"end":{"row":270,"column":60},"placeholder":"..."},{"start":{"row":276,"column":43},"end":{"row":311,"column":37},"placeholder":"..."}],"scrolltop":1743,"scrollleft":0,"selection":{"start":{"row":209,"column":39},"end":{"row":209,"column":39},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":0},"timestamp":1575048257713}